import AdditionalLinks from '@/components/repeated/AdditionalLinks.mdx';

# Custom Endpoints

LibreChat supports OpenAI API compatible services using the `librechat.yaml` configuration file.

This guide assumes you have already set up LibreChat using Docker, as shown in the **[Local Setup Guide](/docs/quick_start/local_setup).**

## Step 1. Create or Edit a Docker Override File

- Create a file named `docker-compose.override.yml` file at the project root (if it doesn't already exist).
- Add the following content to the file:

```yaml
services:
  api:
    volumes:
    - type: bind
      source: ./librechat.yaml
      target: /app/librechat.yaml
```

> Learn more about the [Docker Compose Override File here](/docs/configuration/docker_override).

## Step 2. Configure `librechat.yaml`

- **Create a file named `librechat.yaml`** at the project root (if it doesn't already exist).
- **Add your custom endpoints:** you can view compatible endpoints in the [AI Endpoints section](/docs/configuration/librechat_yaml/ai_endpoints).
  - The list is not exhaustive and generally every OpenAI API-compatible service should work.
  - There are many options for Custom Endpoints. View them all here: [Custom Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/custom_endpoint).
- As an example, here is a configuration for both **OpenRouter** and **Ollama**:

   ```yaml
   version: 1.1.4
   cache: true
   endpoints:
     custom:
       - name: "OpenRouter"
         apiKey: "${OPENROUTER_KEY}"
         baseURL: "https://openrouter.ai/api/v1"
         models:
           default: ["gpt-3.5-turbo"]
           fetch: true
         titleConvo: true
         titleModel: "current_model"
         summarize: false
         summaryModel: "current_model"
         forcePrompt: false
         modelDisplayLabel: "OpenRouter"
       - name: "Ollama"
         apiKey: "ollama"
         baseURL: "http://localhost:11434/v1/"
         models:
           default: [
             "llama3:latest",
             "command-r",
             "mixtral",
             "phi3"
             ]
           fetch: true # fetching list of models is not supported
         titleConvo: true
         titleModel: "current_model"
   ```

## Step 3. Run the App

- Now that your files are configured, you can run the app:

```bash
docker compose up
```

Or, if you were running the app before, you can restart the app with:

```bash
docker compose restart
```

> Note: Make sure your Docker Desktop or Docker Engine is running before executing the command.

---

**That's it!** You have now configured **Custom Endpoints** for your LibreChat instance. 

---

<AdditionalLinks />